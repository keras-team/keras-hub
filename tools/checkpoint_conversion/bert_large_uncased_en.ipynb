{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://github.com/abheesht17/keras-nlp/blob/bert_large_vars/tools/checkpoint_conversion/bert_large_en_uncased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGp_yrJi5Ehf"
   },
   "source": [
    "## Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Szd6xKUd2tIE",
    "outputId": "c5e29c2b-f3ba-46eb-c604-74545a36204f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[K     |████████████████████████████████| 511.7 MB 6.6 kB/s \n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 64.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 63.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 48.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 438 kB 72.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 61.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 63.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 60.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 636 kB 69.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 352 kB 56.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 116 kB 70.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 238 kB 71.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 99 kB 10.0 MB/s \n",
      "\u001b[?25h  Building wheel for keras-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/abheesht17/keras-nlp.git@bert-large-vars tensorflow tf-models-official tensorflow_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JsbnAdSz5DzZ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DmVlNiSexzR7"
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"bert_large\"\n",
    "MODEL_SUFFIX = \"uncased\"\n",
    "MODEL_SPEC_STR = \"L-24_H-1024_A-16\"\n",
    "MODEL_NAME = f\"{MODEL_TYPE}_{MODEL_SUFFIX}\"\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 24\n",
    "NUM_ATTN_HEADS = 16\n",
    "EMBEDDING_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXid57wR3tE5",
    "outputId": "41332d42-8e39-408f-fcc0-803cef5ccfb7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
      "1247797031/1247797031 [==============================] - 23s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# BERT ckpt https://github.com/google-research/bert/blob/master/README.md.\n",
    "zip_path = f\"\"\"https://storage.googleapis.com/bert_models/2018_10_18/{MODEL_SUFFIX}_{MODEL_SPEC_STR}.zip\"\"\"\n",
    "zip_file = keras.utils.get_file(\n",
    "    f\"\"\"/content/{MODEL_NAME}\"\"\",\n",
    "    zip_path,\n",
    "    extract=True,\n",
    "    archive_format=\"zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-VBpV0n4VA3",
    "outputId": "3496afcb-a342-449e-b5b4-975238cec430"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  bert_large_uncased\n",
      "   creating: uncased_L-24_H-1024_A-16/\n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-24_H-1024_A-16/vocab.txt  \n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_config.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"\"\"{MODEL_NAME}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OGij7IQU4rJL"
   },
   "outputs": [],
   "source": [
    "# BERT paths.\n",
    "extract_dir = f\"/content/{MODEL_SUFFIX}_{MODEL_SPEC_STR}\"\n",
    "vocab_path = os.path.join(extract_dir, \"vocab.txt\")\n",
    "checkpoint_path = os.path.join(extract_dir, \"bert_model.ckpt\")\n",
    "config_path = os.path.join(extract_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RC6DqSfo4iPR",
    "outputId": "ba1605fe-4503-49dc-ebc5-d89148527b5d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bert/embeddings/LayerNorm/beta [1024]\n",
      "bert/embeddings/LayerNorm/gamma [1024]\n",
      "bert/embeddings/position_embeddings [512, 1024]\n",
      "bert/embeddings/token_type_embeddings [2, 1024]\n",
      "bert/embeddings/word_embeddings [30522, 1024]\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_0/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_0/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_0/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_0/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_0/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_0/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_0/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_0/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_0/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_0/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_0/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_0/output/dense/bias [1024]\n",
      "bert/encoder/layer_0/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_1/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_1/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_1/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_1/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_1/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_1/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_1/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_1/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_1/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_1/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_1/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_1/output/dense/bias [1024]\n",
      "bert/encoder/layer_1/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_10/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_10/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_10/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_10/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_10/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_10/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_10/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_10/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_10/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_10/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_10/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_10/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_10/output/dense/bias [1024]\n",
      "bert/encoder/layer_10/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_11/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_11/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_11/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_11/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_11/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_11/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_11/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_11/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_11/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_11/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_11/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_11/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_11/output/dense/bias [1024]\n",
      "bert/encoder/layer_11/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_12/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_12/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_12/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_12/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_12/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_12/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_12/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_12/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_12/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_12/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_12/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_12/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_12/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_12/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_12/output/dense/bias [1024]\n",
      "bert/encoder/layer_12/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_13/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_13/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_13/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_13/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_13/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_13/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_13/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_13/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_13/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_13/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_13/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_13/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_13/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_13/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_13/output/dense/bias [1024]\n",
      "bert/encoder/layer_13/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_14/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_14/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_14/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_14/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_14/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_14/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_14/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_14/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_14/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_14/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_14/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_14/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_14/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_14/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_14/output/dense/bias [1024]\n",
      "bert/encoder/layer_14/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_15/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_15/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_15/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_15/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_15/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_15/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_15/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_15/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_15/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_15/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_15/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_15/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_15/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_15/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_15/output/dense/bias [1024]\n",
      "bert/encoder/layer_15/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_16/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_16/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_16/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_16/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_16/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_16/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_16/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_16/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_16/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_16/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_16/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_16/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_16/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_16/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_16/output/dense/bias [1024]\n",
      "bert/encoder/layer_16/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_17/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_17/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_17/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_17/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_17/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_17/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_17/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_17/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_17/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_17/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_17/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_17/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_17/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_17/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_17/output/dense/bias [1024]\n",
      "bert/encoder/layer_17/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_18/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_18/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_18/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_18/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_18/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_18/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_18/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_18/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_18/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_18/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_18/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_18/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_18/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_18/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_18/output/dense/bias [1024]\n",
      "bert/encoder/layer_18/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_19/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_19/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_19/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_19/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_19/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_19/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_19/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_19/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_19/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_19/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_19/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_19/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_19/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_19/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_19/output/dense/bias [1024]\n",
      "bert/encoder/layer_19/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_2/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_2/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_2/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_2/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_2/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_2/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_2/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_2/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_2/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_2/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_2/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_2/output/dense/bias [1024]\n",
      "bert/encoder/layer_2/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_20/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_20/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_20/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_20/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_20/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_20/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_20/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_20/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_20/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_20/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_20/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_20/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_20/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_20/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_20/output/dense/bias [1024]\n",
      "bert/encoder/layer_20/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_21/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_21/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_21/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_21/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_21/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_21/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_21/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_21/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_21/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_21/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_21/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_21/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_21/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_21/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_21/output/dense/bias [1024]\n",
      "bert/encoder/layer_21/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_22/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_22/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_22/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_22/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_22/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_22/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_22/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_22/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_22/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_22/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_22/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_22/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_22/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_22/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_22/output/dense/bias [1024]\n",
      "bert/encoder/layer_22/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_23/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_23/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_23/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_23/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_23/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_23/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_23/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_23/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_23/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_23/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_23/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_23/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_23/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_23/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_23/output/dense/bias [1024]\n",
      "bert/encoder/layer_23/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_3/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_3/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_3/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_3/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_3/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_3/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_3/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_3/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_3/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_3/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_3/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_3/output/dense/bias [1024]\n",
      "bert/encoder/layer_3/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_4/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_4/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_4/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_4/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_4/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_4/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_4/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_4/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_4/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_4/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_4/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_4/output/dense/bias [1024]\n",
      "bert/encoder/layer_4/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_5/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_5/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_5/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_5/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_5/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_5/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_5/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_5/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_5/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_5/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_5/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_5/output/dense/bias [1024]\n",
      "bert/encoder/layer_5/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_6/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_6/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_6/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_6/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_6/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_6/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_6/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_6/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_6/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_6/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_6/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_6/output/dense/bias [1024]\n",
      "bert/encoder/layer_6/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_7/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_7/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_7/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_7/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_7/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_7/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_7/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_7/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_7/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_7/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_7/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_7/output/dense/bias [1024]\n",
      "bert/encoder/layer_7/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_8/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_8/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_8/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_8/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_8/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_8/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_8/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_8/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_8/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_8/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_8/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_8/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_8/output/dense/bias [1024]\n",
      "bert/encoder/layer_8/output/dense/kernel [4096, 1024]\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_9/attention/output/dense/bias [1024]\n",
      "bert/encoder/layer_9/attention/output/dense/kernel [1024, 1024]\n",
      "bert/encoder/layer_9/attention/self/key/bias [1024]\n",
      "bert/encoder/layer_9/attention/self/key/kernel [1024, 1024]\n",
      "bert/encoder/layer_9/attention/self/query/bias [1024]\n",
      "bert/encoder/layer_9/attention/self/query/kernel [1024, 1024]\n",
      "bert/encoder/layer_9/attention/self/value/bias [1024]\n",
      "bert/encoder/layer_9/attention/self/value/kernel [1024, 1024]\n",
      "bert/encoder/layer_9/intermediate/dense/bias [4096]\n",
      "bert/encoder/layer_9/intermediate/dense/kernel [1024, 4096]\n",
      "bert/encoder/layer_9/output/LayerNorm/beta [1024]\n",
      "bert/encoder/layer_9/output/LayerNorm/gamma [1024]\n",
      "bert/encoder/layer_9/output/dense/bias [1024]\n",
      "bert/encoder/layer_9/output/dense/kernel [4096, 1024]\n",
      "bert/pooler/dense/bias [1024]\n",
      "bert/pooler/dense/kernel [1024, 1024]\n",
      "cls/predictions/output_bias [30522]\n",
      "cls/predictions/transform/LayerNorm/beta [1024]\n",
      "cls/predictions/transform/LayerNorm/gamma [1024]\n",
      "cls/predictions/transform/dense/bias [1024]\n",
      "cls/predictions/transform/dense/kernel [1024, 1024]\n",
      "cls/seq_relationship/output_bias [2]\n",
      "cls/seq_relationship/output_weights [2, 1024]\n"
     ]
    }
   ],
   "source": [
    "vars = tf.train.list_variables(checkpoint_path)\n",
    "weights = {}\n",
    "for name, shape in vars:\n",
    "    print(name, shape)\n",
    "    weight = tf.train.load_variable(checkpoint_path, name)\n",
    "    weights[name] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTIwxvcB6hc-"
   },
   "source": [
    "## Load BertLarge model with KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1kp1M9b6hdU",
    "outputId": "8eb94045-3b29-400e-92e7-329cbc1250d7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"bert_custom\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_embedding (Embedding)    (None, None, 1024)   31254528    ['token_ids[0][0]']              \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " position_embedding (PositionEm  (None, None, 1024)  524288      ['token_embedding[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " segment_embedding (Embedding)  (None, None, 1024)   2048        ['segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 1024)   0           ['token_embedding[0][0]',        \n",
      "                                                                  'position_embedding[0][0]',     \n",
      "                                                                  'segment_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " embeddings_layer_norm (LayerNo  (None, None, 1024)  2048        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " embeddings_dropout (Dropout)   (None, None, 1024)   0           ['embeddings_layer_norm[0][0]']  \n",
      "                                                                                                  \n",
      " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_layer_0 (Transform  (None, None, 1024)  12596224    ['embeddings_dropout[0][0]',     \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_1 (Transform  (None, None, 1024)  12596224    ['transformer_layer_0[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_2 (Transform  (None, None, 1024)  12596224    ['transformer_layer_1[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_3 (Transform  (None, None, 1024)  12596224    ['transformer_layer_2[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_4 (Transform  (None, None, 1024)  12596224    ['transformer_layer_3[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_5 (Transform  (None, None, 1024)  12596224    ['transformer_layer_4[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_6 (Transform  (None, None, 1024)  12596224    ['transformer_layer_5[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_7 (Transform  (None, None, 1024)  12596224    ['transformer_layer_6[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_8 (Transform  (None, None, 1024)  12596224    ['transformer_layer_7[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_9 (Transform  (None, None, 1024)  12596224    ['transformer_layer_8[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_10 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_9[0][0]',    \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_11 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_10[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_12 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_11[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_13 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_12[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_14 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_13[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_15 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_14[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_16 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_15[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_17 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_16[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_18 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_17[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_19 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_18[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_20 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_19[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_21 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_20[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_22 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_21[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_23 (Transfor  (None, None, 1024)  12596224    ['transformer_layer_22[0][0]',   \n",
      " merEncoder)                                                      'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1024)        0           ['transformer_layer_23[0][0]']   \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " pooled_dense (Dense)           (None, 1024)         1049600     ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335,141,888\n",
      "Trainable params: 335,141,888\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras_nlp.models.BertLarge(vocabulary_size=VOCAB_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxG_evKB6hdU"
   },
   "source": [
    "## Convert Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VGEx-zLM6hdV"
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"token_embedding\").embeddings.assign(\n",
    "    weights[\"bert/embeddings/word_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"position_embedding\").position_embeddings.assign(\n",
    "    weights[\"bert/embeddings/position_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"segment_embedding\").embeddings.assign(\n",
    "    weights[\"bert/embeddings/token_type_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"embeddings_layer_norm\").gamma.assign(\n",
    "    weights[\"bert/embeddings/LayerNorm/gamma\"]\n",
    ")\n",
    "model.get_layer(\"embeddings_layer_norm\").beta.assign(\n",
    "    weights[\"bert/embeddings/LayerNorm/beta\"]\n",
    ")\n",
    "\n",
    "for i in range(model.num_layers):\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._key_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/key/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._key_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/key/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._query_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/query/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._query_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/query/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._value_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/value/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._value_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/value/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._output_dense.kernel.assign(\n",
    "        weights[\n",
    "            f\"bert/encoder/layer_{i}/attention/output/dense/kernel\"\n",
    "        ].reshape((NUM_ATTN_HEADS, -1, EMBEDDING_SIZE))\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._output_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layernorm.gamma.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/LayerNorm/gamma\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layernorm.beta.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/LayerNorm/beta\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_intermediate_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/intermediate/dense/kernel\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_intermediate_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/intermediate/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_output_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/dense/kernel\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_output_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_layernorm.gamma.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/LayerNorm/gamma\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_layernorm.beta.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/LayerNorm/beta\"]\n",
    "    )\n",
    "\n",
    "model.get_layer(\"pooled_dense\").kernel.assign(\n",
    "    weights[\"bert/pooler/dense/kernel\"]\n",
    ")\n",
    "model.get_layer(\"pooled_dense\").bias.assign(weights[\"bert/pooler/dense/bias\"])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByCEoIyn-_Ld"
   },
   "source": [
    "## Load Bert Large from TF-Hub.\n",
    "\n",
    "These weights have been ratified by the authors of BERT: https://github.com/google-research/bert/blob/master/README.md.\n",
    "\n",
    "### BERT README statement:\n",
    "\n",
    "\"***** New February 7th, 2019: TfHub Module *****\n",
    "BERT has been uploaded to TensorFlow Hub. See run_classifier_with_tfhub.py for an example of how to use the TF Hub module, or run an example in the browser on Colab.\"\n",
    "\n",
    "### TF Hub statement:\n",
    "\"The weights of this model are those released by the original BERT authors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hQ0lMSluxMx1"
   },
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "preprocessor = hub.load(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    ")\n",
    "tokenizer = hub.KerasLayer(preprocessor.tokenize, name=\"tokenizer\")\n",
    "tokenized_text = tokenizer(text_input)\n",
    "\n",
    "packer = hub.KerasLayer(\n",
    "    preprocessor.bert_pack_inputs, arguments=dict(seq_length=512), name=\"packer\"\n",
    ")\n",
    "encoder_inputs = packer([tokenized_text])\n",
    "\n",
    "encoder = hub.KerasLayer(\n",
    "    f\"https://tfhub.dev/tensorflow/bert_en_uncased_{MODEL_SPEC_STR}/4\",\n",
    "    trainable=True,\n",
    ")\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]  # [batch_size, 1024].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 1024].\n",
    "\n",
    "embedding_model = tf.keras.Model(text_input, (pooled_output, sequence_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4OhasjS9Ozn"
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab_path, lowercase=False\n",
    "    )\n",
    "    packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "        sequence_length=model.max_sequence_length,\n",
    "        start_value=tokenizer.token_to_id(\"[CLS]\"),\n",
    "        end_value=tokenizer.token_to_id(\"[SEP]\"),\n",
    "    )\n",
    "    return packer(tokenizer(x))\n",
    "\n",
    "\n",
    "token_ids, segment_ids = preprocess([\"the quick brown fox.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-JvyB96k9qtg"
   },
   "outputs": [],
   "source": [
    "keras_nlp_output = model(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "orig_pooled_output, orig_sequence_output = embedding_model(\n",
    "    tf.constant([\"the quick brown fox.\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzUii8Tp9qth",
    "outputId": "5c7585f3-831e-4da2-abdf-07a968f86856"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([-0.99385583, -0.9897137 ,  0.99999833, -0.99872154, -0.9998986 ,\n",
       "         0.5659573 , -0.9999819 ,  0.9999488 ,  0.9949801 , -0.9976504 ],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([-0.9938567 , -0.9897137 ,  0.99999833, -0.9987216 , -0.9998985 ,\n",
       "         0.5659918 , -0.99998176,  0.99994886,  0.9949803 , -0.99765056],\n",
       "       dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "keras_nlp_output[\"pooled_output\"][0, :10], orig_pooled_output[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "II0akvof9qth",
    "outputId": "0f4d085a-7835-41da-80de-4282fe953f6b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=3.7062455e-07>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.192604e-08>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Very close! Though not 100% exact.\n",
    "(\n",
    "    tf.reduce_mean(keras_nlp_output[\"pooled_output\"] - orig_pooled_output),\n",
    "    tf.reduce_mean(keras_nlp_output[\"sequence_output\"] - orig_sequence_output),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "78sejS0B-Qce"
   },
   "outputs": [],
   "source": [
    "# Save BertLarge checkpoint\n",
    "model.save_weights(f\"\"\"{MODEL_NAME}.h5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bVlbhdZX-QdA"
   },
   "outputs": [],
   "source": [
    "model2 = keras_nlp.models.BertLarge(vocabulary_size=VOCAB_SIZE)\n",
    "model2.load_weights(f\"\"\"{MODEL_NAME}.h5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD0B0UxN-QdB",
    "outputId": "89ffaadc-32ee-4a16-bfc0-6f61f163bc73"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Same output from loaded checkpoint\n",
    "keras_nlp_output2 = model2(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "(\n",
    "    tf.reduce_mean(\n",
    "        keras_nlp_output[\"pooled_output\"] - keras_nlp_output2[\"pooled_output\"]\n",
    "    ),\n",
    "    tf.reduce_mean(\n",
    "        keras_nlp_output[\"sequence_output\"]\n",
    "        - keras_nlp_output2[\"sequence_output\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0K9JAY5-QdD",
    "outputId": "1a4a04e2-ffda-4f68-d396-1490faa1c5e2"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "228209"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Save vocab file as well\n",
    "vocab_info = tf.io.gfile.GFile(vocab_path).read()\n",
    "f = open(\"vocab.txt\", \"w\")\n",
    "f.write(vocab_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jVECpzp-QdD",
    "outputId": "9b05e1b1-adcb-4073-950b-1ab0ab579a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc5cacc9565ef400ee4376105f40ddae  bert_large_uncased.h5\n"
     ]
    }
   ],
   "source": [
    "# Get MD5 of model\n",
    "!md5sum \"\"\"{MODEL_NAME}.h5\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_0iMTCdFl8t"
   },
   "outputs": [],
   "source": [
    "# Upload model to drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTd-5vUyVG0Q",
    "outputId": "b3061cdc-a831-44ae-a6a5-a999b1bf2544"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_large_en_uncased/model.h5\n",
      "1341009960/1341009960 [==============================] - 46s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Check uploaded model once added to repo\n",
    "model_cloud = keras_nlp.models.BertLarge(weights=\"uncased_en\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Same output from cloud model\n",
    "keras_nlp_output_cloud = model_cloud(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")[\"pooled_output\"]\n",
    "tf.reduce_mean(keras_nlp_output[\"pooled_output\"] - keras_nlp_output_cloud)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycvzjqdZYjNo",
    "outputId": "42366183-9d87-4409-a853-de745c38babb"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAwrhAcSzHWa",
    "outputId": "7ff6fcf4-64fb-4f58-c8b3-1bbc05376e85"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([-0.99385583, -0.9897137 ,  0.99999833, -0.99872154, -0.9998986 ,\n",
       "        0.5659573 , -0.9999819 ,  0.9999488 ,  0.9949801 , -0.9976504 ],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "keras_nlp_output_cloud[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "KcwejBTMXsIc"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}