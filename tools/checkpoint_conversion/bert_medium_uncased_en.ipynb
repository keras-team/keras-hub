{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://github.com/abheesht17/keras-nlp/blob/bert_large_vars/tools/checkpoint_conversion/bert_large_en_cased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGp_yrJi5Ehf"
   },
   "source": [
    "## Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Szd6xKUd2tIE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "86338ead-40b4-4149-f30f-8f2b28d3426d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[K     |████████████████████████████████| 511.7 MB 6.1 kB/s \n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 52.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 49.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 55.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 438 kB 68.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 54.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 352 kB 69.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 54.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 99 kB 8.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 238 kB 66.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 116 kB 56.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 636 kB 70.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 58.8 MB/s \n",
      "\u001b[?25h  Building wheel for keras-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/abheesht17/keras-nlp.git@more-bert-variants tensorflow tf-models-official tensorflow_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JsbnAdSz5DzZ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_TYPE = \"bert_medium\"\n",
    "MODEL_SUFFIX = \"uncased\"\n",
    "MODEL_SPEC_STR = \"L-8_H-512_A-8\"\n",
    "MODEL_NAME = f\"{MODEL_TYPE}_{MODEL_SUFFIX}\"\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 8\n",
    "NUM_ATTN_HEADS = 8\n",
    "EMBEDDING_SIZE = 512"
   ],
   "metadata": {
    "id": "DmVlNiSexzR7"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# BERT ckpt https://github.com/google-research/bert/blob/master/README.md.\n",
    "zip_path = f\"\"\"https://storage.googleapis.com/bert_models/2020_02_20/{MODEL_SUFFIX}_{MODEL_SPEC_STR}.zip\"\"\"\n",
    "zip_file = keras.utils.get_file(\n",
    "    f\"\"\"/content/{MODEL_NAME}\"\"\",\n",
    "    zip_path,\n",
    "    extract=True,\n",
    "    archive_format=\"zip\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXid57wR3tE5",
    "outputId": "9acfd0df-e230-4698-9178-08c7e1854fa4"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-512_A-8.zip\n",
      "154608092/154608092 [==============================] - 1s 0us/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip \"\"\"{MODEL_NAME}\"\"\" -d \"\"\"{MODEL_SUFFIX}_{MODEL_SPEC_STR}\"\"\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-VBpV0n4VA3",
    "outputId": "4d6a2bda-7482-42a1-aff5-8a28a6796085"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  bert_medium_uncased\n",
      "  inflating: uncased_L-8_H-512_A-8/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-8_H-512_A-8/bert_config.json  \n",
      "  inflating: uncased_L-8_H-512_A-8/vocab.txt  \n",
      "  inflating: uncased_L-8_H-512_A-8/bert_model.ckpt.index  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# BERT paths.\n",
    "extract_dir = f\"/content/{MODEL_SUFFIX}_{MODEL_SPEC_STR}\"\n",
    "vocab_path = os.path.join(extract_dir, \"vocab.txt\")\n",
    "checkpoint_path = os.path.join(extract_dir, \"bert_model.ckpt\")\n",
    "config_path = os.path.join(extract_dir, \"bert_config.json\")"
   ],
   "metadata": {
    "id": "OGij7IQU4rJL"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vars = tf.train.list_variables(checkpoint_path)\n",
    "weights = {}\n",
    "for name, shape in vars:\n",
    "    print(name, shape)\n",
    "    weight = tf.train.load_variable(checkpoint_path, name)\n",
    "    weights[name] = weight"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RC6DqSfo4iPR",
    "outputId": "c4cc8f57-f252-49fe-a9b6-de96c2162813"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bert/embeddings/LayerNorm/beta [512]\n",
      "bert/embeddings/LayerNorm/gamma [512]\n",
      "bert/embeddings/position_embeddings [512, 512]\n",
      "bert/embeddings/token_type_embeddings [2, 512]\n",
      "bert/embeddings/word_embeddings [30522, 512]\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_0/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_0/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_0/attention/self/key/bias [512]\n",
      "bert/encoder/layer_0/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_0/attention/self/query/bias [512]\n",
      "bert/encoder/layer_0/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_0/attention/self/value/bias [512]\n",
      "bert/encoder/layer_0/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_0/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_0/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_0/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_0/output/dense/bias [512]\n",
      "bert/encoder/layer_0/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_1/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_1/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_1/attention/self/key/bias [512]\n",
      "bert/encoder/layer_1/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_1/attention/self/query/bias [512]\n",
      "bert/encoder/layer_1/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_1/attention/self/value/bias [512]\n",
      "bert/encoder/layer_1/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_1/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_1/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_1/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_1/output/dense/bias [512]\n",
      "bert/encoder/layer_1/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_2/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_2/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_2/attention/self/key/bias [512]\n",
      "bert/encoder/layer_2/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_2/attention/self/query/bias [512]\n",
      "bert/encoder/layer_2/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_2/attention/self/value/bias [512]\n",
      "bert/encoder/layer_2/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_2/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_2/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_2/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_2/output/dense/bias [512]\n",
      "bert/encoder/layer_2/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_3/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_3/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_3/attention/self/key/bias [512]\n",
      "bert/encoder/layer_3/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_3/attention/self/query/bias [512]\n",
      "bert/encoder/layer_3/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_3/attention/self/value/bias [512]\n",
      "bert/encoder/layer_3/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_3/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_3/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_3/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_3/output/dense/bias [512]\n",
      "bert/encoder/layer_3/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_4/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_4/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_4/attention/self/key/bias [512]\n",
      "bert/encoder/layer_4/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_4/attention/self/query/bias [512]\n",
      "bert/encoder/layer_4/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_4/attention/self/value/bias [512]\n",
      "bert/encoder/layer_4/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_4/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_4/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_4/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_4/output/dense/bias [512]\n",
      "bert/encoder/layer_4/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_5/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_5/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_5/attention/self/key/bias [512]\n",
      "bert/encoder/layer_5/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_5/attention/self/query/bias [512]\n",
      "bert/encoder/layer_5/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_5/attention/self/value/bias [512]\n",
      "bert/encoder/layer_5/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_5/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_5/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_5/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_5/output/dense/bias [512]\n",
      "bert/encoder/layer_5/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_6/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_6/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_6/attention/self/key/bias [512]\n",
      "bert/encoder/layer_6/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_6/attention/self/query/bias [512]\n",
      "bert/encoder/layer_6/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_6/attention/self/value/bias [512]\n",
      "bert/encoder/layer_6/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_6/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_6/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_6/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_6/output/dense/bias [512]\n",
      "bert/encoder/layer_6/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_7/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_7/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_7/attention/self/key/bias [512]\n",
      "bert/encoder/layer_7/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_7/attention/self/query/bias [512]\n",
      "bert/encoder/layer_7/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_7/attention/self/value/bias [512]\n",
      "bert/encoder/layer_7/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_7/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_7/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_7/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_7/output/dense/bias [512]\n",
      "bert/encoder/layer_7/output/dense/kernel [2048, 512]\n",
      "bert/pooler/dense/bias [512]\n",
      "bert/pooler/dense/kernel [512, 512]\n",
      "cls/predictions/output_bias [30522]\n",
      "cls/predictions/transform/LayerNorm/beta [512]\n",
      "cls/predictions/transform/LayerNorm/gamma [512]\n",
      "cls/predictions/transform/dense/bias [512]\n",
      "cls/predictions/transform/dense/kernel [512, 512]\n",
      "cls/seq_relationship/output_bias [2]\n",
      "cls/seq_relationship/output_weights [2, 512]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTIwxvcB6hc-"
   },
   "source": [
    "## Load BertMedium model with KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c4c615a2-2997-4682-a8fa-97da40289e23",
    "id": "g1kp1M9b6hdU"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"bert_custom\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_embedding (Embedding)    (None, None, 512)    15627264    ['token_ids[0][0]']              \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " position_embedding (PositionEm  (None, None, 512)   262144      ['token_embedding[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " segment_embedding (Embedding)  (None, None, 512)    1024        ['segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 512)    0           ['token_embedding[0][0]',        \n",
      "                                                                  'position_embedding[0][0]',     \n",
      "                                                                  'segment_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " embeddings_layer_norm (LayerNo  (None, None, 512)   1024        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " embeddings_dropout (Dropout)   (None, None, 512)    0           ['embeddings_layer_norm[0][0]']  \n",
      "                                                                                                  \n",
      " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_layer_0 (Transform  (None, None, 512)   3152384     ['embeddings_dropout[0][0]',     \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_1 (Transform  (None, None, 512)   3152384     ['transformer_layer_0[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_2 (Transform  (None, None, 512)   3152384     ['transformer_layer_1[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_3 (Transform  (None, None, 512)   3152384     ['transformer_layer_2[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_4 (Transform  (None, None, 512)   3152384     ['transformer_layer_3[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_5 (Transform  (None, None, 512)   3152384     ['transformer_layer_4[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_6 (Transform  (None, None, 512)   3152384     ['transformer_layer_5[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_7 (Transform  (None, None, 512)   3152384     ['transformer_layer_6[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 512)         0           ['transformer_layer_7[0][0]']    \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " pooled_dense (Dense)           (None, 512)          262656      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,373,184\n",
      "Trainable params: 41,373,184\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras_nlp.models.BertMedium(vocabulary_size=VOCAB_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxG_evKB6hdU"
   },
   "source": [
    "## Convert Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VGEx-zLM6hdV"
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"token_embedding\").embeddings.assign(\n",
    "    weights[\"bert/embeddings/word_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"position_embedding\").position_embeddings.assign(\n",
    "    weights[\"bert/embeddings/position_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"segment_embedding\").embeddings.assign(\n",
    "    weights[\"bert/embeddings/token_type_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"embeddings_layer_norm\").gamma.assign(\n",
    "    weights[\"bert/embeddings/LayerNorm/gamma\"]\n",
    ")\n",
    "model.get_layer(\"embeddings_layer_norm\").beta.assign(\n",
    "    weights[\"bert/embeddings/LayerNorm/beta\"]\n",
    ")\n",
    "\n",
    "for i in range(model.num_layers):\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._key_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/key/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._key_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/key/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._query_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/query/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._query_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/query/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._value_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/value/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._value_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/value/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._output_dense.kernel.assign(\n",
    "        weights[\n",
    "            f\"bert/encoder/layer_{i}/attention/output/dense/kernel\"\n",
    "        ].reshape((NUM_ATTN_HEADS, -1, EMBEDDING_SIZE))\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._output_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layernorm.gamma.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/LayerNorm/gamma\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layernorm.beta.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/LayerNorm/beta\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_intermediate_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/intermediate/dense/kernel\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_intermediate_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/intermediate/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_output_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/dense/kernel\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_output_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_layernorm.gamma.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/LayerNorm/gamma\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_layernorm.beta.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/LayerNorm/beta\"]\n",
    "    )\n",
    "\n",
    "model.get_layer(\"pooled_dense\").kernel.assign(\n",
    "    weights[\"bert/pooler/dense/kernel\"]\n",
    ")\n",
    "model.get_layer(\"pooled_dense\").bias.assign(weights[\"bert/pooler/dense/bias\"])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Bert Medium from TF-Hub.\n",
    "\n",
    "These weights have been ratified by the authors of BERT: https://github.com/google-research/bert/blob/master/README.md.\n",
    "\n",
    "### BERT README statement:\n",
    "\n",
    "\"***** New February 7th, 2019: TfHub Module *****\n",
    "BERT has been uploaded to TensorFlow Hub. See run_classifier_with_tfhub.py for an example of how to use the TF Hub module, or run an example in the browser on Colab.\""
   ],
   "metadata": {
    "id": "ByCEoIyn-_Ld"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "preprocessor = hub.load(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    ")\n",
    "tokenizer = hub.KerasLayer(preprocessor.tokenize, name=\"tokenizer\")\n",
    "tokenized_text = tokenizer(text_input)\n",
    "\n",
    "packer = hub.KerasLayer(\n",
    "    preprocessor.bert_pack_inputs, arguments=dict(seq_length=512), name=\"packer\"\n",
    ")\n",
    "encoder_inputs = packer([tokenized_text])\n",
    "\n",
    "encoder = hub.KerasLayer(\n",
    "    f\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_{MODEL_SPEC_STR}/2\",\n",
    "    trainable=True,\n",
    ")\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]  # [batch_size, 1024].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 1024].\n",
    "\n",
    "embedding_model = tf.keras.Model(text_input, (pooled_output, sequence_output))"
   ],
   "metadata": {
    "id": "hQ0lMSluxMx1"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess(x):\n",
    "    tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab_path, lowercase=False\n",
    "    )\n",
    "    packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "        sequence_length=model.max_sequence_length,\n",
    "        start_value=tokenizer.token_to_id(\"[CLS]\"),\n",
    "        end_value=tokenizer.token_to_id(\"[SEP]\"),\n",
    "    )\n",
    "    return packer(tokenizer(x))\n",
    "\n",
    "\n",
    "token_ids, segment_ids = preprocess([\"the quick brown fox.\"])"
   ],
   "metadata": {
    "id": "iAubWsWj9qtg"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-JvyB96k9qtg"
   },
   "outputs": [],
   "source": [
    "keras_nlp_output = model(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "hub_pooled_output, hub_sequence_output = embedding_model(\n",
    "    tf.constant([\"the quick brown fox.\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "keras_nlp_output[\"pooled_output\"][0, :10], hub_pooled_output[0, :10]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "06d8485d-84f0-47ea-eab9-7d229da581ce",
    "id": "HzUii8Tp9qth"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.02575585, -0.03279256, -0.9999834 , -0.8720043 ,  0.9662866 ,\n",
       "        -0.01399663, -0.9921592 , -0.01616581, -0.29182166,  0.02937486],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.02575908, -0.03278941, -0.9999834 , -0.8720044 ,  0.96628505,\n",
       "        -0.01399296, -0.99215966, -0.01616178, -0.29182625,  0.02937203],\n",
       "       dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1fbc2ea7-577f-491d-97d1-2becf294eb94",
    "id": "II0akvof9qth"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-1.911394e-07>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.966175e-08>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Very close! Though not 100% exact.\n",
    "(\n",
    "    tf.reduce_mean(keras_nlp_output[\"pooled_output\"] - hub_pooled_output),\n",
    "    tf.reduce_mean(keras_nlp_output[\"sequence_output\"] - hub_sequence_output),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "78sejS0B-Qce"
   },
   "outputs": [],
   "source": [
    "# Save BertMedium checkpoint\n",
    "model.save_weights(f\"\"\"{MODEL_NAME}.h5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bVlbhdZX-QdA"
   },
   "outputs": [],
   "source": [
    "model2 = keras_nlp.models.BertMedium(vocabulary_size=VOCAB_SIZE)\n",
    "model2.load_weights(f\"\"\"{MODEL_NAME}.h5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0c3ae2e3-f23e-46a3-a7bc-2b9e29e72a21",
    "id": "OD0B0UxN-QdB"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Same output from loaded checkpoint\n",
    "keras_nlp_output2 = model2(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "(\n",
    "    tf.reduce_mean(\n",
    "        keras_nlp_output[\"pooled_output\"] - keras_nlp_output2[\"pooled_output\"]\n",
    "    ),\n",
    "    tf.reduce_mean(\n",
    "        keras_nlp_output[\"sequence_output\"]\n",
    "        - keras_nlp_output2[\"sequence_output\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "68d4f6b4-dd02-423c-ca45-c18ede23a3dc",
    "id": "q0K9JAY5-QdD"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "228209"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Save vocab file as well\n",
    "vocab_info = tf.io.gfile.GFile(vocab_path).read()\n",
    "f = open(\"vocab.txt\", \"w\")\n",
    "f.write(vocab_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0e9970fb-cb39-4255-c1bb-65d891a8b845",
    "id": "-jVECpzp-QdD"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bb990e1184ec6b6185450c73833cd661  bert_medium_uncased.h5\n"
     ]
    }
   ],
   "source": [
    "# Get MD5 of model\n",
    "!md5sum \"\"\"{MODEL_NAME}.h5\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_0iMTCdFl8t"
   },
   "outputs": [],
   "source": [
    "# Upload model to drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTd-5vUyVG0Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "083acd06-b55f-4b07-82e4-3f71f866500e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_large_en_cased/model.h5\n",
      "1334759464/1334759464 [==============================] - 41s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Check uploaded model once added to repo\n",
    "model_cloud = keras_nlp.models.BertMedium(weights=\"uncased_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zs5x_f6GVdNY",
    "outputId": "9ea2098f-4c71-4d8c-9991-6672b1de9f34"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Same output from cloud model\n",
    "keras_nlp_output_cloud = model_cloud(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")[\"pooled_output\"]\n",
    "tf.reduce_mean(keras_nlp_output[\"pooled_output\"] - keras_nlp_output_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "keras_nlp_output_cloud[0, :10]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAwrhAcSzHWa",
    "outputId": "92e1ecc4-b783-4f60-f65f-c2895ba1218f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([ 0.97578144,  0.9996469 ,  0.9997959 , -0.94946283,  0.99925387,\n",
       "        0.9986442 , -0.9969186 , -0.9611691 ,  0.99938154,  0.9999203 ],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "S2JGnbTYaeGc"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}