{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://github.com/abheesht17/keras-nlp/blob/bert_large_vars/tools/checkpoint_conversion/bert_large_en_cased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGp_yrJi5Ehf"
   },
   "source": [
    "## Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Szd6xKUd2tIE",
    "outputId": "33a180e3-462f-4f6a-b641-0104f18f96de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 511.7 MB 6.8 kB/s \n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 48.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 48.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 52.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 438 kB 64.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 56.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 238 kB 68.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 352 kB 38.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 116 kB 74.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 99 kB 10.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 50.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 56.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 636 kB 69.5 MB/s \n",
      "\u001b[?25h  Building wheel for keras-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/abheesht17/keras-nlp.git@more-bert-variants tensorflow tf-models-official tensorflow_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JsbnAdSz5DzZ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DmVlNiSexzR7"
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"bert_small\"\n",
    "MODEL_SUFFIX = \"uncased\"\n",
    "MODEL_SPEC_STR = \"L-4_H-512_A-8\"\n",
    "MODEL_NAME = f\"{MODEL_TYPE}_{MODEL_SUFFIX}\"\n",
    "VOCAB_SIZE = 30522\n",
    "NUM_LAYERS = 4\n",
    "NUM_ATTN_HEADS = 8\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXid57wR3tE5",
    "outputId": "37545bbe-70bc-4824-f96e-3eda6b99e709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-512_A-8.zip\n",
      "107814641/107814641 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# BERT ckpt https://github.com/google-research/bert/blob/master/README.md.\n",
    "zip_path = f\"\"\"https://storage.googleapis.com/bert_models/2020_02_20/{MODEL_SUFFIX}_{MODEL_SPEC_STR}.zip\"\"\"\n",
    "zip_file = keras.utils.get_file(\n",
    "    f\"\"\"/content/{MODEL_NAME}\"\"\",\n",
    "    zip_path,\n",
    "    extract=True,\n",
    "    archive_format=\"zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-VBpV0n4VA3",
    "outputId": "8ca6d690-8f73-45e7-a0e9-16b5901a4297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  bert_small_uncased\n",
      "  inflating: uncased_L-4_H-512_A-8/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-4_H-512_A-8/bert_config.json  \n",
      "  inflating: uncased_L-4_H-512_A-8/vocab.txt  \n",
      "  inflating: uncased_L-4_H-512_A-8/bert_model.ckpt.index  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"\"\"{MODEL_NAME}\"\"\" -d \"\"\"{MODEL_SUFFIX}_{MODEL_SPEC_STR}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OGij7IQU4rJL"
   },
   "outputs": [],
   "source": [
    "# BERT paths.\n",
    "extract_dir = f\"/content/{MODEL_SUFFIX}_{MODEL_SPEC_STR}\"\n",
    "vocab_path = os.path.join(extract_dir, \"vocab.txt\")\n",
    "checkpoint_path = os.path.join(extract_dir, \"bert_model.ckpt\")\n",
    "config_path = os.path.join(extract_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RC6DqSfo4iPR",
    "outputId": "405b2156-e025-4848-8aee-2589c4354311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/embeddings/LayerNorm/beta [512]\n",
      "bert/embeddings/LayerNorm/gamma [512]\n",
      "bert/embeddings/position_embeddings [512, 512]\n",
      "bert/embeddings/token_type_embeddings [2, 512]\n",
      "bert/embeddings/word_embeddings [30522, 512]\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_0/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_0/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_0/attention/self/key/bias [512]\n",
      "bert/encoder/layer_0/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_0/attention/self/query/bias [512]\n",
      "bert/encoder/layer_0/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_0/attention/self/value/bias [512]\n",
      "bert/encoder/layer_0/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_0/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_0/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_0/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_0/output/dense/bias [512]\n",
      "bert/encoder/layer_0/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_1/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_1/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_1/attention/self/key/bias [512]\n",
      "bert/encoder/layer_1/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_1/attention/self/query/bias [512]\n",
      "bert/encoder/layer_1/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_1/attention/self/value/bias [512]\n",
      "bert/encoder/layer_1/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_1/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_1/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_1/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_1/output/dense/bias [512]\n",
      "bert/encoder/layer_1/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_2/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_2/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_2/attention/self/key/bias [512]\n",
      "bert/encoder/layer_2/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_2/attention/self/query/bias [512]\n",
      "bert/encoder/layer_2/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_2/attention/self/value/bias [512]\n",
      "bert/encoder/layer_2/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_2/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_2/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_2/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_2/output/dense/bias [512]\n",
      "bert/encoder/layer_2/output/dense/kernel [2048, 512]\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_3/attention/output/dense/bias [512]\n",
      "bert/encoder/layer_3/attention/output/dense/kernel [512, 512]\n",
      "bert/encoder/layer_3/attention/self/key/bias [512]\n",
      "bert/encoder/layer_3/attention/self/key/kernel [512, 512]\n",
      "bert/encoder/layer_3/attention/self/query/bias [512]\n",
      "bert/encoder/layer_3/attention/self/query/kernel [512, 512]\n",
      "bert/encoder/layer_3/attention/self/value/bias [512]\n",
      "bert/encoder/layer_3/attention/self/value/kernel [512, 512]\n",
      "bert/encoder/layer_3/intermediate/dense/bias [2048]\n",
      "bert/encoder/layer_3/intermediate/dense/kernel [512, 2048]\n",
      "bert/encoder/layer_3/output/LayerNorm/beta [512]\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma [512]\n",
      "bert/encoder/layer_3/output/dense/bias [512]\n",
      "bert/encoder/layer_3/output/dense/kernel [2048, 512]\n",
      "bert/pooler/dense/bias [512]\n",
      "bert/pooler/dense/kernel [512, 512]\n",
      "cls/predictions/output_bias [30522]\n",
      "cls/predictions/transform/LayerNorm/beta [512]\n",
      "cls/predictions/transform/LayerNorm/gamma [512]\n",
      "cls/predictions/transform/dense/bias [512]\n",
      "cls/predictions/transform/dense/kernel [512, 512]\n",
      "cls/seq_relationship/output_bias [2]\n",
      "cls/seq_relationship/output_weights [2, 512]\n"
     ]
    }
   ],
   "source": [
    "vars = tf.train.list_variables(checkpoint_path)\n",
    "weights = {}\n",
    "for name, shape in vars:\n",
    "    print(name, shape)\n",
    "    weight = tf.train.load_variable(checkpoint_path, name)\n",
    "    weights[name] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTIwxvcB6hc-"
   },
   "source": [
    "## Load BertSmall model with KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1kp1M9b6hdU",
    "outputId": "24c6056a-ef5a-426c-b172-f5aeec699fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_custom\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_embedding (Embedding)    (None, None, 512)    15627264    ['token_ids[0][0]']              \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " position_embedding (PositionEm  (None, None, 512)   262144      ['token_embedding[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " segment_embedding (Embedding)  (None, None, 512)    1024        ['segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 512)    0           ['token_embedding[0][0]',        \n",
      "                                                                  'position_embedding[0][0]',     \n",
      "                                                                  'segment_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " embeddings_layer_norm (LayerNo  (None, None, 512)   1024        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " embeddings_dropout (Dropout)   (None, None, 512)    0           ['embeddings_layer_norm[0][0]']  \n",
      "                                                                                                  \n",
      " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_layer_0 (Transform  (None, None, 512)   3152384     ['embeddings_dropout[0][0]',     \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_1 (Transform  (None, None, 512)   3152384     ['transformer_layer_0[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_2 (Transform  (None, None, 512)   3152384     ['transformer_layer_1[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_layer_3 (Transform  (None, None, 512)   3152384     ['transformer_layer_2[0][0]',    \n",
      " erEncoder)                                                       'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 512)         0           ['transformer_layer_3[0][0]']    \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " pooled_dense (Dense)           (None, 512)          262656      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,763,648\n",
      "Trainable params: 28,763,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras_nlp.models.BertSmall(vocabulary_size=VOCAB_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxG_evKB6hdU"
   },
   "source": [
    "## Convert Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VGEx-zLM6hdV"
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"token_embedding\").embeddings.assign(\n",
    "    weights[\"bert/embeddings/word_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"position_embedding\").position_embeddings.assign(\n",
    "    weights[\"bert/embeddings/position_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"segment_embedding\").embeddings.assign(\n",
    "    weights[\"bert/embeddings/token_type_embeddings\"]\n",
    ")\n",
    "model.get_layer(\"embeddings_layer_norm\").gamma.assign(\n",
    "    weights[\"bert/embeddings/LayerNorm/gamma\"]\n",
    ")\n",
    "model.get_layer(\"embeddings_layer_norm\").beta.assign(\n",
    "    weights[\"bert/embeddings/LayerNorm/beta\"]\n",
    ")\n",
    "\n",
    "for i in range(model.num_layers):\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._key_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/key/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._key_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/key/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._query_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/query/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._query_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/query/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._value_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/value/kernel\"].reshape(\n",
    "            (EMBEDDING_SIZE, NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._value_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/self/value/bias\"].reshape(\n",
    "            (NUM_ATTN_HEADS, -1)\n",
    "        )\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._output_dense.kernel.assign(\n",
    "        weights[\n",
    "            f\"bert/encoder/layer_{i}/attention/output/dense/kernel\"\n",
    "        ].reshape((NUM_ATTN_HEADS, -1, EMBEDDING_SIZE))\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layer._output_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layernorm.gamma.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/LayerNorm/gamma\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._self_attention_layernorm.beta.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/attention/output/LayerNorm/beta\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_intermediate_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/intermediate/dense/kernel\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_intermediate_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/intermediate/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_output_dense.kernel.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/dense/kernel\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_output_dense.bias.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/dense/bias\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_layernorm.gamma.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/LayerNorm/gamma\"]\n",
    "    )\n",
    "    model.get_layer(\n",
    "        f\"transformer_layer_{i}\"\n",
    "    )._feedforward_layernorm.beta.assign(\n",
    "        weights[f\"bert/encoder/layer_{i}/output/LayerNorm/beta\"]\n",
    "    )\n",
    "\n",
    "model.get_layer(\"pooled_dense\").kernel.assign(\n",
    "    weights[\"bert/pooler/dense/kernel\"]\n",
    ")\n",
    "model.get_layer(\"pooled_dense\").bias.assign(weights[\"bert/pooler/dense/bias\"])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByCEoIyn-_Ld"
   },
   "source": [
    "## Load Bert Small from TF-Hub.\n",
    "\n",
    "These weights have been ratified by the authors of BERT: https://github.com/google-research/bert/blob/master/README.md.\n",
    "\n",
    "### BERT README statement:\n",
    "\n",
    "\"***** New February 7th, 2019: TfHub Module *****\n",
    "BERT has been uploaded to TensorFlow Hub. See run_classifier_with_tfhub.py for an example of how to use the TF Hub module, or run an example in the browser on Colab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hQ0lMSluxMx1"
   },
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "preprocessor = hub.load(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    ")\n",
    "tokenizer = hub.KerasLayer(preprocessor.tokenize, name=\"tokenizer\")\n",
    "tokenized_text = tokenizer(text_input)\n",
    "\n",
    "packer = hub.KerasLayer(\n",
    "    preprocessor.bert_pack_inputs, arguments=dict(seq_length=512), name=\"packer\"\n",
    ")\n",
    "encoder_inputs = packer([tokenized_text])\n",
    "\n",
    "encoder = hub.KerasLayer(\n",
    "    f\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_{MODEL_SPEC_STR}/2\",\n",
    "    trainable=True,\n",
    ")\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]  # [batch_size, 1024].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 1024].\n",
    "\n",
    "embedding_model = tf.keras.Model(text_input, (pooled_output, sequence_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iAubWsWj9qtg"
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab_path, lowercase=False\n",
    "    )\n",
    "    packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "        sequence_length=model.max_sequence_length,\n",
    "        start_value=tokenizer.token_to_id(\"[CLS]\"),\n",
    "        end_value=tokenizer.token_to_id(\"[SEP]\"),\n",
    "    )\n",
    "    return packer(tokenizer(x))\n",
    "\n",
    "\n",
    "token_ids, segment_ids = preprocess([\"the quick brown fox.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-JvyB96k9qtg"
   },
   "outputs": [],
   "source": [
    "keras_nlp_output = model(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "hub_pooled_output, hub_sequence_output = embedding_model(\n",
    "    tf.constant([\"the quick brown fox.\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzUii8Tp9qth",
    "outputId": "97bd5f3c-8440-4f86-f87d-2940370463e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.99953026,  0.7659703 ,  0.00867063,  0.4091944 , -0.48858136,\n",
       "         0.8769211 ,  0.9966225 , -0.98421854, -0.4603666 , -0.75314736],\n",
       "       dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.9995303 ,  0.7659644 ,  0.00867085,  0.40919945, -0.48858863,\n",
       "         0.8769245 ,  0.9966227 , -0.9842189 , -0.4603672 , -0.75314724],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nlp_output[\"pooled_output\"][0, :10], hub_pooled_output[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "II0akvof9qth",
    "outputId": "e7b49897-abb3-45da-9ad8-45a62b22a37d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-5.848767e-08>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-6.5012145e-08>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very close! Though not 100% exact.\n",
    "(\n",
    "    tf.reduce_mean(keras_nlp_output[\"pooled_output\"] - hub_pooled_output),\n",
    "    tf.reduce_mean(keras_nlp_output[\"sequence_output\"] - hub_sequence_output),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "78sejS0B-Qce"
   },
   "outputs": [],
   "source": [
    "# Save BertSmall checkpoint\n",
    "model.save_weights(f\"\"\"{MODEL_NAME}.h5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bVlbhdZX-QdA"
   },
   "outputs": [],
   "source": [
    "model2 = keras_nlp.models.BertSmall(vocabulary_size=VOCAB_SIZE)\n",
    "model2.load_weights(f\"\"\"{MODEL_NAME}.h5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD0B0UxN-QdB",
    "outputId": "6d4feea6-bc13-4fc2-a9b0-33e929ff9d0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same output from loaded checkpoint\n",
    "keras_nlp_output2 = model2(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "(\n",
    "    tf.reduce_mean(\n",
    "        keras_nlp_output[\"pooled_output\"] - keras_nlp_output2[\"pooled_output\"]\n",
    "    ),\n",
    "    tf.reduce_mean(\n",
    "        keras_nlp_output[\"sequence_output\"]\n",
    "        - keras_nlp_output2[\"sequence_output\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0K9JAY5-QdD",
    "outputId": "697099b1-de1e-4393-a272-12144d29d155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save vocab file as well\n",
    "vocab_info = tf.io.gfile.GFile(vocab_path).read()\n",
    "f = open(\"vocab.txt\", \"w\")\n",
    "f.write(vocab_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jVECpzp-QdD",
    "outputId": "7e75dcb5-b985-4b44-801c-e8d63d34f035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08632c9479b034f342ba2c2b7afba5f7  bert_small_uncased.h5\n"
     ]
    }
   ],
   "source": [
    "# Get MD5 of model\n",
    "!md5sum \"\"\"{MODEL_NAME}.h5\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_0iMTCdFl8t"
   },
   "outputs": [],
   "source": [
    "# Upload model to drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTd-5vUyVG0Q",
    "outputId": "083acd06-b55f-4b07-82e4-3f71f866500e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_large_en_cased/model.h5\n",
      "1334759464/1334759464 [==============================] - 41s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Check uploaded model once added to repo\n",
    "model_cloud = keras_nlp.models.BertSmall(weights=\"uncased_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zs5x_f6GVdNY",
    "outputId": "9ea2098f-4c71-4d8c-9991-6672b1de9f34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same output from cloud model\n",
    "keras_nlp_output_cloud = model_cloud(\n",
    "    {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"segment_ids\": segment_ids,\n",
    "        \"padding_mask\": token_ids != 0,\n",
    "    }\n",
    ")[\"pooled_output\"]\n",
    "tf.reduce_mean(keras_nlp_output[\"pooled_output\"] - keras_nlp_output_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAwrhAcSzHWa",
    "outputId": "92e1ecc4-b783-4f60-f65f-c2895ba1218f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([ 0.97578144,  0.9996469 ,  0.9997959 , -0.94946283,  0.99925387,\n",
       "        0.9986442 , -0.9969186 , -0.9611691 ,  0.99938154,  0.9999203 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nlp_output_cloud[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2JGnbTYaeGc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
